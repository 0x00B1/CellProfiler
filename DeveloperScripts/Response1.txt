[[I've put our notes to ourselves in brackets, and the real, proposed response outside of brackets.  "Ray: ..." is written by Ray, all else by Anne.]]
[[NOTE: the response to some sections of the reviews might simply say 'we have addressed all the minor comments' rather than going through one by one, but at this stage it's good to itemize what our changes to the text will be. I'd recommend we write out everything we are going to do here before we begin messing with the manuscript itself.]]

Response to Reviewers' Comments, and guide to changes in the manuscript:  


Reviewer #1:

Summary
The authors describe a supervised machine learning approach to score phenotypes in high-throughput image-based screens. The approach requires human experts to provide examples of cells with and without phenotypes of interest. A machine learning algorithm is then trained to recognize the provided cells, and opinions from human experts are used iteratively to improve the classification accuracy.

Major Comments
COMMENT 1A: There is an extensive prior works in the area of automated high throughput cellular phenotype profiling using microscopy images. The statement that (past) "automated image analysis requires extensive customization to each phenotype ..." is not true. For example, unbiased sets of measurements that are not customized for any particular phenotypes were used in Boland, et al., Bioinformatics (2001), Tanaka, et. al., PLOS Biology (2005), and Loo, et al., Nature Methods (2007).

Response:
Agreed, our meaning was unclear in that sentence. We replaced the sentence with: "Automated image analysis coupled with machine learning can effectively score many phenotypes but can require significant effort to adapt to rare or subtle morphologies, particularly when positive control samples are not available."


COMMENT 1B: Further more, the authors' claim that "none has been implemented in a system proven capable of scoring millions of cells for many diverse morphologies - especially for phenotypes that are complex or subtle, without known positive controls, or present in only a small percentage of the cells in a sample". However, it appear that, as many other prior works, the proposed method also requires positive samples provided from human experts. Thus, it is unclear what do the authors mean by "without known positive controls"?

Response:
The distinction between positive control samples and positive control cells was not clear in our original sentence. To clarify, we have replaced the sentence with: "These methods require providing examples of cells that do and do not display the morphology of interest (i.e., positive and negative cells). However, finding a sufficient number of positive cells can be prohibitively difficult when positive control samples are not available and when the phenotype is rare in the wild-type population."


COMMENT 1C: The idea of using human experts to guide classifier is neither novel nor interesting. The proposed approach of identifying positive control cells from "cells noticed by chance while browsing images from the screen, or outlier cells discovered by interactively exploring cell measurements" is highly biased, non-systematic, and, worst of all, probabilistic (i.e., may not be reproducible). Although the proposed method is good at "zooming" into the phenotypes selected by human experts, it is unclear how the proposed system may discover novel cells or phenotypes that are missed by the human experts.

Response:
The reviewer misunderstands the goal of the work we described - the goal is phenotype scoring, not phenotype discovery. The sentence was describing how a researcher might choose an interesting phenotype that they wish to pursue; historically, selecting phenotypes for study has indeed been highly biased (i.e., guided by the intuition of a biologist), in classic screens that have been the basis of much of modern biology. Methods exist for automated phenotype discovery, but that is not addressed by our work. We have clarified this in a revision of the sentence: "These can be cells from control samples if the screen has been designed to address a particular phenotype. These can also be cells identified by chance or based on outlier measurements if the screen’s goal is to uncover previously unknown or uncharacterized phenotypes, which was the case in this study."


COMMENT 1D: This manuscript lacks methodological novelty or biological discovery to make this appropriate for Nature Methods.

Response:
There are no prior publications demonstrating the use of software for supervised, iterative machine learning to carry out a large-scale screen. The methodology has proven useful in the study described as well as several other screens completed in other groups collaborating with us.

[[Ray: I think we can leave it alone.  In fact, I think we can leave almost all of reviewer 1 alone. Anne: are the overall responses to reviewer 1 short enough now? We might omit response to 1D, although then I think it's awkward because we either leave the reviewer's sentence out (which seems like we are hiding it) or we leave a blank where the response should be (which seems like we want to let it stand as true). What do you think?]]

===========

Reviewer #2:

This ms reports a new software tool, Classifier, that the authors have implemented within their existing CellAnalyst package.  Classifier is aptly named-it is a tool that provides a nice, apparently easy-to-use interface for running sophisticated feature-based supervised learning.  

There are a few successes in this field, but none to my knowledge that are routinely applied, distributed and available.  The authors should be commended for their efforts and commitment to developing useful and available software.  To my knowledge, this is unique in the image classification domain, and an important contribution for the field.  

COMMENT 2A: However, does this very useful implementation constitute a significant methodological or conceptual advance?  This is a surprisingly light treatment of an important topic that the authors properly and critically introduce-the use of automatic, supervised learning methods in large-scale cell-based screens.  As such the authors don't reach the standard achieved by other published papers in this field- reports of confusion matrices (as opposed to the very simple right/wrong analysis provided here), discussion of different classifier implementations, examination of feature space explored by different phenotypes. The authors' statement that "...automated scoring of unusual phenotypes in general can now be accomplished, if accurate cytoprofiles can be generated for the cell images" doesn't seem novel.  Admittedly, many other papers from Eils, Murphy, Ellenberg & Pepperkok, and many others referenced by the authors have focused on automatic identification of human-recognized phenotypes, but Murphy's group have already established good methods for  distinguishing phenotypes based on molecular identity that were not known to be distinguishable by humans-in short classifiers can find new phenotypes.  Against this background, the authors' contention of a significant conceptual advance seems dubious. 

Response:
The referee points out particular explorations that would be helpful to evaluate the methods we have described:
(a) Confusion matrices: 
[[Should we just convert the forced choice to a confusion matrix (which would look very very nice)? That would probably not please the referee, unless we make the argument that we have to use forced choice to keep the human experts scoring roughly the same number of positives and negatives and not having a different threshold of what they consider a 'hit'. We could provide a standard confusion matrix on a per-cell basis, of course (right?), or point to the supplemental figures showing sensitivity and specificity (that's essentially the same thing, right?), but what to do about per-image?

Ray:  We can report confusion matrices per-cell.  Per-sample we don't have a lot of data, as we lack ground truth except a small fraction of samples, so I'd rather avoid that.

TO DO: Let's discuss.]]

(b) Discussion of different classifier implementations:
Evaluation of various classifier methods for cell screens has indeed been a popular focus of study and has been heavily explored in others' work, particularly from the Murphy lab. One conclusion from this work is that none of the top classification methods (e.g., [[X, Y, GentleBoosting]]) consistently performs dramatically better than the others [[Ray, is that accurate?]]. The goal of the approach we present is a method for rapidly creating useful training sets for rare phenotypes; comparing actual classifier methods is orthogonal to this work. To clarify this, we have added a sentence [[somewhere near where we mention GentleBoosting, saying something like "Other machine learning methods are also likely to be effective, based on their performance in previous work (Murphy lab references)"]].

[[Can we just throw the proposed best (i.e., something that was top of the list in a murphy paper) one into CPA and test it? This would also address the modularity, for part 2C.3.

Ray: I'd rather cut the modularity claim than go down the path of evaluating classifier methods, which are very orthogonal to this work.  I'd rather address this by being explicit about not testing other classifier methods, but feeling that they'd do fine.

Anne: I think your response is ok, but if it takes one day to implement and test the 'best' alternate machine learning method on one phenotype, I'd like to try it. It's always risky to tell the reviewer you don't want to do their proposed experiment, especially if it's clearly not that time-consuming. Perhaps Vebjorn could carry this out while we are working on other parts?  Note also that our modularity claim was actually more along the lines of - You can swap in dift image processing, feature extraction, or machine learning methods. I think those are not overstating the claims because CP is a separate step from CPA so inherently that's modular (so long as it really is easy to put in a new m-l method into CPA)]]

(c) Examination of feature space explored by different phenotypes:

[[Do you suppose that they want just a general narrative about what features are helpful? or we could make a heatmappy thing showing what features were used for which phenotypes (I think Altschuler papers have had this), or we could make a bar chart with four bars for each phenotype: size, shape, intensity, texture. Then categorize the 611 features into one of those categories and count up how many rules were in each of those 4 categories. This little 4-bar chart per phenotype would be feasible to add to the existing big figures, I think. But a heatmap is easier to make and possibly more sophisticated-looking and could be a suppl. figure.

Ray: I don't think the feature space comment needs to be or should be addressed directly.  That's a much more complicated idea than what we're proposing, and very dependent on the feature set we measure.

Anne: Again, I think it's not worth the risk to ignore this comment, especially when it's so easy to make the two types of plots I propose. We should pick our battles and be willing to do things that don't make sense even if they take us a few extra days in total.]]

(d) The referee is concerned that "...automated scoring of unusual phenotypes in general can now be accomplished, if accurate cytoprofiles can be generated for the cell images" doesn't seem novel.  
[[Is this concern addressed by just saying 'can now be accomplished with a few hours of setup time'? Are those other papers similar to the method we used in the suppl figure (the one we informally called the 'bakal' approach)?  See 1A.  Murphy method hasn't been used in any real experiments? ok, that's a little harsh. but has it been used by any outside groups in the past decade? that might be actually literally true.

Ray: I think emphasizing the speed and flexibility might be the best way to go.  Murphy's methods and others' could have handled any of the phtypes we looked at, but it would have been far more painful to build the training set, etc.

TODO: Let's discuss.]]


COMMENT 2B: The authors are first-class scientists, therefore it's worth posing a hypothetical question--  if I submitted a paper to Nature Methods reporting a new method based on an application of that method to a single dataset, would they consider that a reasonable demonstration of the general utility of the method?  This report describes the use of Classifier on one cell-based screen.  That implementation is certainly an achievement, but the authors (and editors of this journal, who have been guilty of this!) must consider the impact of reporting results based on a single run, in the absence of any significant finding derived from this analysis.  This does not diminish the work the authors have done, but recognizes the importance of delivering tested, generally useful methodology to the community.  The use of Classifier to define phenotypes across multiple screens, with multiple probes is therefore most important. Even surveying the types of assays most often done in HCS, and critically evaluating the phenotypes that are well detected and those that are less well-detected would be sufficient.  The authors must have access to this data.

Response:
Indeed, several other projects using Classifier have been successfully completed but not yet published. Three in particular have conducted large-scale screens so far: (a) Piyush Gupta in Eric Lander's lab has led completion of an RNAi screen for regulators of the subtle cytoskeletal changes induced by a growth factor on T47D human breast cancer cells, (b) Taio Xie in Tim Mitchison's lab has led completion of two genome-wide RNAi screens in HeLa cells for spindle morphology during cell division, and (c) Adam Castereno in Riki Eggert's lab has processed a substantial proportion of images from a chemical screen in Drosophila Kc167 cells to identify modulators of a binucleate phenotype. These large-scale experiments probed phenotypes distinct from each other and from those explored in our own screen. They were conducted by different investigators, in different cell types/organisms, using different image acquisition devices at different magnifications. These screens are, of course, intended for separate publication so we have not added detailed description to this manuscript - in fact, two are now awaiting acceptance of our own manuscript before they submit their own work so as not to scoop us with our own method!

[[INCLUDE A FIGURE OF THESE PHENOTYPES]]

[[Anne will send our short response to Daniel & ask whether he finds this convincing, or whether we should do a quick new screen for a fly phenotype or some other human phenotype, e.g., using piyush's images. We can tell him that doing a new screen would be fine, but work exterior to this paper (and with real scientific goals) carries more weight.]]
[[Note also that we mentioned Yaffe & Floyd in our original paragraph but Anne is doublechecking whether they've actually used Classifier in their screen yet. Re. Gilliland, we haven't mentioned them and probably should not add them because it's a bit too soon.]]

COMMENT 2C: Moreover, a few practical but very important considerations are missing:
COMMENT 2C.1: No report on the computational performance of Classifier is mentioned.  How long does training take?  How does this scale with number of images, channels, image size, etc.?  What kinds of hardware are used for this calculation.  The authors have a very impressive compute facility at their disposal-is this required? Perhaps the application is limited by I/O, given its emphasis on large numbers of images.  Reporting these limitations is absolutely essential. 

Response:

[[This seems straightforward; Ray, can you draft a new paragraph/sentences for the text?

Ray: Yeah, it's easy to address this, as most of the times are trivial, except for image processing, which is secondary to the paper.

Anne: agreed, we probably don't need to mention the image processing times at all because the reviewer isn't asking about those (or is that evasive?)

TODO: Ray will do]]


COMMENT 2C.2: The authors have bypassed any description of cellAnalyst, in favor of delivering a separate publication on this, but simple statements like support for commercial file formats, necessary configuration, etc. would be most valuable.  Many HCS platforms have a mix of data acquisition systems-which can be used?

Response:
Good idea. We added: "CellProfiler and CellProfiler Analyst are compatible with images acquired on most commercial microscopes, including high-throughput automated microscopes (e.g., Cellomics, Molecular Devices, Zeiss). Adaptation of CellProfiler Analyst to a particular data set involves adjusting a properties file indicating the configuration of the database containing the measurements (CellProfiler Analyst, www.cellprofiler.org, TR Jones, IH Kang, DB Wheeler, RA Lindquist, A Papallo, DM Sabatini, P Golland, AE Carpenter, submitted)."


COMMENT 2C.3: I will admit to a personal pet peeve, as a user with over 20 years of experience in digital imaging in the life sciences.  All software is called by its authors easy to use, and almost always easily pluggable.  Yet, in fact there are only a few examples where this is in fact the case, as judged by uptake from the community- ImageJ is probably the best example in life sciences imaging (there are many others in other scientific domains, e.g., crystallography).  The authors' assertion of an modular architecture (p. 9)  should be at least illustrated with pointers to where examples and documentation for this exist, and preferably with examples of how to add new classifiers.

Response:
This is certainly a reasonable pet peeve to hold in this field!  Of course, it is difficult to establish metrics to substantiate claims of 'easy-to-use', but we submit that many of our collaborators currently use the tool to score large-scale screens independently. Often, training was minimal (Mitchison & Lander groups), and in fact we have recently released an even easier-to-use interface for Classifier. The online demo shows this new Classifier tool in use (http://www.cellprofiler.com/examples) and we hope the referee will agree that the process is easy based on that evidence as well. [[Probably exclude this sentence: "We have not adjusted the text, but would be happy to do so if there seems a better way to substantiate ease of use."]]

[[Ray: I think we should just cut the pluggable part.  Evaluating pluggability is even harder than ease-of-use, and we don't have anything to back up the claim with.  It's not key to what we wrote, so backing it out should be fine.]]
[[TODO: wait to decide about pluggable to see if we test an alternate machine learning method. Also, there's confusion about modularity of CP vs CPA.]]


===========

Reviewer #3:
COMMENT 3A: A central argument of this manuscript is that computer vision techniques can be applied to visual cell-based screens in order to find subtle phenotypes in an objective way.  The paper describes an iterative manual process to build a set of rules by manually correcting feedback from the classifier as it is being built.  Since this process is based on the user's own perception, isn't the entire process then limited by the user's ability to discern the phenotypes as well as any bias the user may introduce?  Is there a way of building and refining the classifier based solely on experimental controls without relying on a user's visual perception?  There are several examples in the literature where human observers are unable to distinguish sub-cellular morphologies that are known to be different (for example, fluorescently labeled lysosomes and endosomes).

Response: 
[[Yes, but that's a different goal. Some chemical/RNAi's have multiple effects on cells so excluding the biologist's intervention will lead down the wrong path. Both types of tools are needed, and our approach is needed when only some of the effects of positive control samples are of biological interest.

Ray: I'm not sure what the "Some chemical/RNAi's..." sentence is trying to address (bias?).  I think we address this by saying, "yes, positive/negative controls allow more direct scoring, but we're doing something else."  Clarification in the paper may be necessary?]]]]


COMMENT 3B: This is not the first time that pattern recognition was used to evaluate a set of diverse cellular phenotypes.  This was demonstrated a decade ago by Robert Murphy's group for determining sub-cellular locations (Cytometry 1998).  Although on the face of it this addresses a different biological problem (identification of sub-cellular compartment), the visual analysis problem is the same because the classifier doesn't "know" its identifying sub-cellular locations vs. RNAi phenotypes - in both cases the problem is to identify a set of diverse cellular morphologies.  Since Murphy's work, others have built single classifiers for solving a diversity of biological and cellular imaging problems (e.g., Neumann et al., Bakal et al., Orlov et al., Chen & Murphy, as well as Loo et al. in a recent Nature Methods ).  In each of these approaches, classifiers are built using an automated process where the underlying image features are not problem-specific, making these approaches applicable to diverse imaging problems.  In light of this, the statement "none has addressed the problem of scale in terms of ability to score cells for many unusual phenotypes of interest" is not correct.  

Response:


COMMENT 3C: The novelty in the proposed approach is the focus on low-penetrance phenotypes, the motivation to make this software easy to use for biologists as well as allowing for iterative/interactive manual classifier building and refinement.

Response:
[[None needed, i think]]


COMMENT 3D: A more detailed description of the image features used  is necessary.  While it may be beyond the scope of this journal to describe these algorithms in detail, the descriptions provided for what is being measured about each cell are inadequate to evaluate how the software operates.  This is equally true of the classifier algorithm itself.  GentleBoosting is  a method of combining several weak classifiers or a method of weighting a collection of image features.  It is not a classification algorithm on its own.  The actual algorithm(s) used for classification is not mentioned.  

Response:


COMMENT 3E: The description of the classifier evaluation is also inadequate, but extremely important for this type of publication.  How many individual images were evaluated by both the classifier and manually?  How were the test images presented to manual scorers chosen - were they selected by the classifier?  It is not possible to evaluate the false positive and false negative rates presented without a more thorough description of the evaluation procedure.  Lastly, it would be informative to compare the performance of this classifier to other existing classification methods.  What are the limitations of this technique? How many different phenotypes can be handled in the same experiment? How does the performance degrade when the number of rules increases? What types of phenotypes are undetectable by  Classify? 

Response:


COMMENT 3F: A very real danger in applying pattern recognition techniques to visual assays is over-training.  In this application, this would manifest as an ability to robustly discern phenotypes within a set of images, but an inability to "generalize" this even to a repetition of the same experiment on a different day.  The performance of this classifier when applied to biological replicates should be looked into and discussed.

Response:
We will address two separate underlying concerns: First, overtraining on a single data set was not observed (Figure [[showing plots showing the error function not decreasing after adding more rules]]), which is to be expected because GentleBoosting and other boosting algorithms are in general resistant to overtraining ([[REF]]).  We could include this as a supplementary figure, but we hesitate to lengthen the manuscript. 

Second, we address generalizing a classifier to an image set acquired in a separate experiment. This is not usually  necessary because retraining is easy relative to the effort of replicating a large-scale screen and it would be preferable to create a training set for all replicates together. Regardless, 

(1)  .(3) And it works in at least some cases (Figure, piyush data, but we can't publish it probably).

These issues are briefly summarized by a new addition to the manuscript: "XXXX".

[[Daniel: I'm happy to hear that you believe you will be able to address the
concerns of the refs regarding evaluation of the performance of Analyst.
We weren't sure how well you would be able to address the concern of
overtraining unless the method is somehow resistant to this. ]]


COMMENT 3G: A central criticism of this work is that the underlying basis is an image classification technique, but the paper focuses almost exclusively on its implementation in a software tool.  This is somewhat justified in that the software tool is indeed unique:  Despite previous work by others in this specific application of pattern recognition and image classification, no one has focused as much effort on making their software useable by bench scientists.  Whether this is sufficiently novel for publication in Nature Methods is an editorial decision.

Response:
[[no repsonse needed?]]


COMMENT 3H: Additional detailed comments:
COMMENT 3H.1: Page 5. It is mentioned that 611 measurements are used without mention of what these are or even what types of measurements are involved. Some analysis could also be useful, but at the very least users should be able to know what is being measured and how. I couldn't find them in the enclosed additional papers, and also not in the CellProfiler paper published in "Genome Biology". The image analysis is one of the most important contributions of this work, and it must be described.

Response:
[[submit PIPE as suppl data, plus say it's all open source software so they can see exactly what we do, plus provide text list of the 611 features as suppl data. in general, too many papers are cluttered with pages of lists of features when they aren't really novel. ok, maybe we shouldn't say that latter statement.]]


COMMENT 3H.2: Related to the previous comment. In page 4 ("Results"), several different characteristic are measured as a first step of the analysis. However, except from specifying that these include size, shape, intensity and textures, no information describing these measurements is provided. For instance, what characteristics of the shapes are used? How are they measured? The reference (27) does not lead to a source that specifies that. Intensity can be measured by very many different ways. Is it measured by using the statistical properties of the pixel intensity distribution? If so, which statistical properties are used? How are they measured? The term "texture" is also a very broad definition. Many techniques for measuring textures have been proposed, and the paper must describe which methods are used and how, or at least include references in which the exact same fashion of the use of these textures is specified. The rule files also feature some other measurements such as edges and polynomial decomposition. These should also be described in the text.

Response:


COMMENT 3H.3: Another major point is the rules. It is clear that the rules are determined by the researcher in an interactive fashion using trial and error until she is satisfied with the results.  These rules, as provided in supplementary materials need further explanation.  The rule files seem to be a set of "if" conditions. However, the manuscript does not describe how the if clauses are handled, and it is not clear how the inference computation is performed and how the ranking is determined.  It is also not clear how important the manual correction of the rules is, or if the rules can be used without manual correction. What is the contribution of the manual correction (in terms of classification accuracy) to the automatically generated rules. This point is important because the readers might be curious to know if the method can be fully automated.

Response:


COMMENT 3H.4: Just by looking at the rule files, it seems that some of the image content descriptors that are used are computationally intensive. A few words about the computational resources that are required and the response time of the system might be useful for researchers who consider using this method, especially on the very large datasets (10s - 100s of thousands of images) one would encounter in a screen.

Response:


COMMENT 3H.5: Page 10:  "50 individual rules" ==> "Fifty individual rules". (A sentence should not start with a number).

Response:


COMMENT 3H.6: Illustrations: All graphs must have clear units on both axes.

Response:


COMMENT 3H.7: It is totally unclear how gentleboosting is used. Boosting requires ground truth and an existing classifier (as opposed to filtering, which requires only ground truth). Since the boosting is applied here as a first step of defining the classifier rules, it is unclear what classifier is used by gentleboosting (decision trees?). It should also be mentioned that the classifier used for boosting is different than the one used for the actual classification, and therefore features that are selected might not be informative for the actual classifier. Under these circumstances, it is unclear why the authors chose to use slow boosting over much faster and classifier-independent filtering.
The number of terms used by gentleboosting is also unclear. Is this number fixed or is it determined dynamically in some way?
While gentleboosting assigns weights to the features, it is unclear how the tentative rule is generated.
Also, boosting is usually a computationally intensive task. A note about this issue is also required.

Response:


COMMENT 3H.8: Page 5: "Few dozens" is a rough estimation.  How does a researcher determine the number? Usually, the performance of machine learning algorithms can be improved by using more samples for training. This trivial practice of machine learning might not be known to many biologists. To help the readers effectively use the described tool, this must be further discussed, preferably by measuring the effect of several different training-set sizes on classifier performance.

Response:


COMMENT 3H.9: The term "scoring" is used throughout the manuscript in the sense of associating a specific sample with one of a discrete set of classes. This, however, is actually "classification", while the term "scoring" implies on some continuos indicator, e.g., Z-scores, FIsher scores, etc.

Response:


COMMENT 3H.10: Discussion: Since only a subset of the dataset (reference 12) was used, the reader might wonder what was the barrier for repeating the same experiment for much larger network than the 14 phenotypes. What were the criteria for choosing these phenotypes.

Response:


COMMENT 3H.11: Page 9: The statement according which Classifier can be used on full organisms such as zebrafish and C. elegans is not supported by any empirical or theorethical evidence. The authors can use some of the publicly available datasets and report on the detection rate. Unless this is done, such a statement should be left out of the manuscript.

Response:
[[fine by me to leave it out. It's pretty conventional to talk about future directions for the work, so we could also re-word it, but I would assume it was already pretty clearly future directions and they still didn't like it so it might be simpler to just leave it out.]]


COMMENT 3H.12: Validation and comparison to previous methods: It is clear that Classifier was compared to human detection, but the way the human detection was performed is not clear. Was each sample classified by a single researcher? What is the error rate of this manual classification? How many samples were used for training the classifier? What efforts (human resources) were required to train the classifier.

Response:


COMMENT 3H.13: The motivation for the forced choice test is unclear. Is there a specific reason for using this kind of test, rather than simply counting the number of samples that their automated score was in agreement with the manual score?

Response:


COMMENT 3H.14: RNAi screens for diverse phenotypes using Classifier: What are the "standard cytoprofiles? Is there a list of the cell descriptors?


Response:


COMMENT 3H.15: Also, a new descriptor (the angle between the two nearest neighbors) was added. It is not clear, however, if and how the user can add customized descriptors.

Response:
[[Two mechanisms: (1) add more modules to the image analysis pipeline (this is easy to do in CellProfiler and does not require programming skills, simply add a module to the pipeline. An example would be if there is a particular compartment to be defined, e.g. a concentric ring from 2-6 microns outside the edge of the nucleus, that might be helpful to score a particular phenotype. (2) write new modules for CellProfiler, in matlab. This is the route we took for angle between neighbors, because it was not a pre-existing measurement that CellProfiler was enabled to make. It is now a standard measurement included in the standard cytoprofile for future screens.]]


COMMENT 3H. 16: Page 3: "Machine learning methods that select and combine multiple features for automated cell scoring have been
explored and used for a few particular phenotypes"  The cited papers do not support the statement. Previous efforts (e.g., Neumann et al., Bakal et al., Orlov et al., Chen & Murphy) have resulted in general phenotype recognition tools that can handle a large number of phenotypes, rather than focusing on detection of a few particular phenotypes. This is an important point, and it should be clear to the reader that this work is not the first attempt of general automated phenotype recognition.

Response:

