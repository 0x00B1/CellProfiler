Response to Reviewers' Comments, and guide to changes in the manuscript:  

Reviewer #1:

Summary
The authors describe a supervised machine learning approach to score phenotypes in high-throughput image-based screens. The approach requires human experts to provide examples of cells with and without phenotypes of interest. A machine learning algorithm is then trained to recognize the provided cells, and opinions from human experts are used iteratively to improve the classification accuracy.

Major Comments
COMMENT 1A: There is an extensive prior works in the area of automated high throughput cellular phenotype profiling using microscopy images. The statement that (past) "automated image analysis requires extensive customization to each phenotype ..." is not true. For example, unbiased sets of measurements that are not customized for any particular phenotypes were used in Boland, et al., Bioinformatics (2001), Tanaka, et. al., PLOS Biology (2005), and Loo, et al., Nature Methods (2007).

Response:
Agreed, our meaning was unclear in that sentence. We replaced the sentence with: "Automated image analysis coupled with machine learning can effectively score many phenotypes but can require significant effort to adapt to rare or subtle morphologies, particularly when positive control samples are not available."


COMMENT 1B: Further more, the authors' claim that "none has been implemented in a system proven capable of scoring millions of cells for many diverse morphologies - especially for phenotypes that are complex or subtle, without known positive controls, or present in only a small percentage of the cells in a sample". However, it appear that, as many other prior works, the proposed method also requires positive samples provided from human experts. Thus, it is unclear what do the authors mean by "without known positive controls"?

Response:
The distinction between positive control samples and positive control cells was not clear in our original sentence. To clarify, we have replaced the sentence with: "These methods require providing examples of cells that do and do not display the morphology of interest (i.e., positive and negative cells). However, finding a sufficient number of positive cells can be prohibitively difficult when positive control samples are not available and when the phenotype is rare in the wild-type population."


COMMENT 1C: The idea of using human experts to guide classifier is neither novel nor interesting. The proposed approach of identifying positive control cells from "cells noticed by chance while browsing images from the screen, or outlier cells discovered by interactively exploring cell measurements" is highly biased, non-systematic, and, worst of all, probabilistic (i.e., may not be reproducible). Although the proposed method is good at "zooming" into the phenotypes selected by human experts, it is unclear how the proposed system may discover novel cells or phenotypes that are missed by the human experts.

Response:
The goal of our work is phenotype scoring, not discovery of novel phenotypes. The sentence mentioned above describes how a researcher might choose an interesting phenotype that they wish to pursue; historically, selecting phenotypes for study has indeed been highly biased (i.e., guided by the intuition of a biologist), particularly in classic screens that have been the basis of much of modern biology. Methods exist for automated phenotype discovery, but this is not addressed by our work. We have revised the sentence to clarify: "These can be cells from control samples if the screen has been designed to address a particular phenotype, or cells identified by chance if the screen’s goal is to uncover previously uncharacterized phenotypes, as in many classic screens."


COMMENT 1D: This manuscript lacks methodological novelty or biological discovery to make this appropriate for Nature Methods.

Response:
Please see our response to Reviewer 3, comment 3H.

===========

Reviewer #2:

This ms reports a new software tool, Classifier, that the authors have implemented within their existing CellAnalyst package.  Classifier is aptly named-it is a tool that provides a nice, apparently easy-to-use interface for running sophisticated feature-based supervised learning.  

There are a few successes in this field, but none to my knowledge that are routinely applied, distributed and available.  The authors should be commended for their efforts and commitment to developing useful and available software.  To my knowledge, this is unique in the image classification domain, and an important contribution for the field.  

COMMENT 2A: However, does this very useful implementation constitute a significant methodological or conceptual advance?  This is a surprisingly light treatment of an important topic that the authors properly and critically introduce-the use of automatic, supervised learning methods in large-scale cell-based screens.  As such the authors don't reach the standard achieved by other published papers in this field- reports of confusion matrices (as opposed to the very simple right/wrong analysis provided here), discussion of different classifier implementations, examination of feature space explored by different phenotypes. The authors' statement that "...automated scoring of unusual phenotypes in general can now be accomplished, if accurate cytoprofiles can be generated for the cell images" doesn't seem novel.  Admittedly, many other papers from Eils, Murphy, Ellenberg & Pepperkok, and many others referenced by the authors have focused on automatic identification of human-recognized phenotypes, but Murphy's group have already established good methods for  distinguishing phenotypes based on molecular identity that were not known to be distinguishable by humans-in short classifiers can find new phenotypes.  Against this background, the authors' contention of a significant conceptual advance seems dubious. 

Response:
The referee points out particular explorations that would be helpful to evaluate the methods we have described. We left these out of the original manuscript to maintain sharp focus, but they are now included in the revised version:
(a) Confusion matrices (Supplementary Figure [[XXX - confusion matrix]]) support the validation results originally shown in Figure 3 ("Validation" column).  In addition to the existing statement that there were no false negatives and, in total, two false positives across the 720 samples scored by eye, we have added this sentence: "Agreement between humans was comparable to that between humans and automated scoring (Supplementary Figure [[XXX - confusion matrix]])."

(b) Discussion of different classifier implementations:
Evaluation of various classifier algorithms has indeed been heavily explored in others' work, both for cell screens in the Murphy lab, as well as for many other domains in the general machine learning community. One conclusion from this work is that the top classification methods (e.g., SVMs, neural networks, boosting variants) perform roughly similarly.  Contributing to the work comparing particular learning algorithms is not our goal; rather, we present a method for rapidly creating useful training sets for rare phenotypes, using an arbitrary machine learning algorithm. In fact, we believe that any of these methods would perform well within our framework - our initial testing indicated that SVMs performed similarly to GentleBoosting on decision stumps but we chose the latter because SVMs were less suitable for generating the database queries necessary for scoring entire screens. To clarify this, we have added a sentence: "Other machine learning methods are likely to be equally effective, based on their performance in previous work [[(Murphy lab, mitocheck, altschuler, bakal refs XXX)]]."

(c) Examination of feature space explored by different phenotypes:
We have created a Supplementary Figure [[XXX]] to facilitate examination of the feature space explored by each phenotype.
[[TO DO: Make this figure, either lumping together into ~6 categories, or a skinny bar chart of all 611 features. Reference this figure in the text where we mention features, near 'standard cytoprofiles' discussion.]]

[[Question to Bjorn/Vebjorn about (a-c): here, we address the reviewer's specific queries, but are we addressing the reviewer's overall concerns adequately? Anything else we should add?]]

(d) The referee is concerned that our statement "...automated scoring of unusual phenotypes in general can now be accomplished, if accurate cytoprofiles can be generated for the cell images" doesn't seem novel.  
We agree that the original sentence unintentionally implied that we applied machine learning to cell screening for the first time. We have adjusted the sentence to appropriately qualify the statement: "automated scoring of a wide variety of morphologies can now be accomplished quickly and easily, even when the phenotype is rare in the wild-type population and positive control samples are not available."

COMMENT 2B: The authors are first-class scientists, therefore it's worth posing a hypothetical question--  if I submitted a paper to Nature Methods reporting a new method based on an application of that method to a single dataset, would they consider that a reasonable demonstration of the general utility of the method?  This report describes the use of Classifier on one cell-based screen.  That implementation is certainly an achievement, but the authors (and editors of this journal, who have been guilty of this!) must consider the impact of reporting results based on a single run, in the absence of any significant finding derived from this analysis.  This does not diminish the work the authors have done, but recognizes the importance of delivering tested, generally useful methodology to the community.  The use of Classifier to define phenotypes across multiple screens, with multiple probes is therefore most important. Even surveying the types of assays most often done in HCS, and critically evaluating the phenotypes that are well detected and those that are less well-detected would be sufficient.  The authors must have access to this data.

Response:
The reviewer makes an excellent point. Indeed, several other projects using Classifier have been successful but not yet published. We have now included analysis of a second image set (a different screen with different stains by different investigators) in response to Reviewer 3's comment 3G about over-training and how to score replicates performed on different days (please see that response below). Here, we should also elaborate on the screens mentioned only briefly in the text: (a) Piyush Gupta in Eric Lander's lab has led completion of an RNAi screen for regulators of the subtle cytoskeletal changes induced by a growth factor on T47D human breast cancer cells, (b) Taio Xie in Tim Mitchison's lab has led completion of two genome-wide RNAi screens in HeLa cells for spindle morphology during cell division, and (c) Adam Castereno in Riki Eggert's lab has processed a substantial proportion of images from a chemical screen in Drosophila Kc167 cells to identify modulators of a binucleate phenotype. These large-scale experiments probe phenotypes distinct from each other and from those explored in our own screens described in this manuscript. They were conducted by different investigators, in different cell types/organisms, using different image acquisition devices at different magnifications. These screens are, of course, intended for separate publication so we have not added detailed description to this manuscript.

[[Anne TO DO: Make a figure for phenotypes (a-c) just for the rebuttal letter, not for the manuscript.]]


COMMENT 2C: Moreover, a few practical but very important considerations are missing:
COMMENT 2C.1: No report on the computational performance of Classifier is mentioned.  How long does training take?  How does this scale with number of images, channels, image size, etc.?  What kinds of hardware are used for this calculation.  The authors have a very impressive compute facility at their disposal-is this required? Perhaps the application is limited by I/O, given its emphasis on large numbers of images.  Reporting these limitations is absolutely essential. 

Response:

[[TODO: Ray will write. most of the times are trivial, except for image processing, which is secondary to the paper, but we will still mention image processing briefly.]]


COMMENT 2C.2: The authors have bypassed any description of cellAnalyst, in favor of delivering a separate publication on this, but simple statements like support for commercial file formats, necessary configuration, etc. would be most valuable.  Many HCS platforms have a mix of data acquisition systems-which can be used?

Response:
Additional information in the main text would indeed be helpful. We added: "CellProfiler and CellProfiler Analyst are compatible with images acquired on most commercial microscopes, including high-throughput automated microscopes (e.g., Cellomics, Molecular Devices, Zeiss). Configuring CellProfiler Analyst to access a new CellProfiler-generated database of measurements involves minor adjustments to a properties file (CellProfiler Analyst, www.cellprofiler.org, TR Jones, IH Kang, DB Wheeler, RA Lindquist, A Papallo, DM Sabatini, P Golland, AE Carpenter, submitted)."


COMMENT 2C.3: I will admit to a personal pet peeve, as a user with over 20 years of experience in digital imaging in the life sciences.  All software is called by its authors easy to use, and almost always easily pluggable.  Yet, in fact there are only a few examples where this is in fact the case, as judged by uptake from the community- ImageJ is probably the best example in life sciences imaging (there are many others in other scientific domains, e.g., crystallography).  The authors' assertion of an modular architecture (p. 9)  should be at least illustrated with pointers to where examples and documentation for this exist, and preferably with examples of how to add new classifiers.

Response:
We concur. It is difficult to establish metrics to substantiate claims of 'easy-to-use', but we submit that many of our collaborators currently use the tool to score large-scale screens independently, usually with minimal training.  We have recently released a new interface for Classifier intended to further improve ease-of-use. The online demo shows this new Classifier tool in use (http://www.cellprofiler.com/examples). Thus, we do indeed consider this tool to be user-friendly, although we do not make this claim in the manuscript.

Regarding modularity, we agree that demonstrated use by a broad community of researchers is strong evidence. Such evidence exists for CellProfiler image analysis software (not described in this manuscript), which has been adapted to dozens of projects by researchers unaffiliated with our group, but because CellProfiler Analyst/Classifier was released only recently, we decided to omit all text at the end of the discussion describing future possibilities, including substituting novel machine learning algorithms.

===========

Reviewer #3:
COMMENT 3A: A central argument of this manuscript is that computer vision techniques can be applied to visual cell-based screens in order to find subtle phenotypes in an objective way.  The paper describes an iterative manual process to build a set of rules by manually correcting feedback from the classifier as it is being built.  Since this process is based on the user's own perception, isn't the entire process then limited by the user's ability to discern the phenotypes as well as any bias the user may introduce?  Is there a way of building and refining the classifier based solely on experimental controls without relying on a user's visual perception?  There are several examples in the literature where human observers are unable to distinguish sub-cellular morphologies that are known to be different (for example, fluorescently labeled lysosomes and endosomes).

Response: 
Yes, previous work (e.g., the work of the Murphy Lab on sub-cellular localization) has demonstrated that if positive controls are available and the phenotype is highly penetrant, computers can be trained to discriminate between phenotypes that cannot be visually distinguished by humans, by labeling all cells from positive control samples as positive examples and all cells from negative control samples as negative examples. Classifier provides a convenient interface for performing this type of analysis, by loading all cells from the controls into the training set without any interactive manual selection of cells.  In our work, we address the more challenging situation where the phenotype is less penetrant, positive controls are not available, or both, which makes generating training sets difficult. The failure of methods that do not leverage the user's visual perception to score such phenotypes (but instead label all positive control samples' cells as positives) is documented in the original Supplementary Figure [[XXX - bakal-like figure]].

The researcher-guided definition of a phenotype has another advantage that we did not sufficiently emphasize in the original manuscript: even when positive control samples are available, training to the phenotype of a control may not [[Anne TODO ... Clean up all this to finish the thought: Some chemical/RNAi's have multiple effects on cells so excluding the biologist's intervention will lead down the wrong path. Both types of tools are needed, and our approach is needed when only some of the effects of positive control samples are of biological interest.]]

[[Anne TODO: need to decide where to put clarification of all this in the revised manuscript.]]


COMMENT 3B: This is not the first time that pattern recognition was used to evaluate a set of diverse cellular phenotypes.  This was demonstrated a decade ago by Robert Murphy's group for determining sub-cellular locations (Cytometry 1998).  Although on the face of it this addresses a different biological problem (identification of sub-cellular compartment), the visual analysis problem is the same because the classifier doesn't "know" its identifying sub-cellular locations vs. RNAi phenotypes - in both cases the problem is to identify a set of diverse cellular morphologies.  Since Murphy's work, others have built single classifiers for solving a diversity of biological and cellular imaging problems (e.g., Neumann et al., Bakal et al., Orlov et al., Chen & Murphy, as well as Loo et al. in a recent Nature Methods ).  In each of these approaches, classifiers are built using an automated process where the underlying image features are not problem-specific, making these approaches applicable to diverse imaging problems.  In light of this, the statement "none has addressed the problem of scale in terms of ability to score cells for many unusual phenotypes of interest" is not correct.  

Response:
[[Anne TODO: need to see if comment 1A and 1B address this. Probably need to add penetrance qualifier to the original sentence: "Thus, while recent software has solved the problem of scale in terms of ability to analyze hundreds of thousands of samples for particular phenotypes, none has addressed the problem of scale in terms of ability to score cells for many unusual phenotypes of interest."]]

COMMENT 3C: The novelty in the proposed approach is the focus on low-penetrance phenotypes, the motivation to make this software easy to use for biologists as well as allowing for iterative/interactive manual classifier building and refinement.

Response:
Yes, and we have adjusted the text to more clearly convey these advancements (in particular, see responses to comment 1A, 1B, and 3H).


COMMENT 3D: A more detailed description of the image features used  is necessary.  While it may be beyond the scope of this journal to describe these algorithms in detail, the descriptions provided for what is being measured about each cell are inadequate to evaluate how the software operates.  This is equally true of the classifier algorithm itself.  GentleBoosting is  a method of combining several weak classifiers or a method of weighting a collection of image features.  It is not a classification algorithm on its own.  The actual algorithm(s) used for classification is not mentioned.  

Response:
We regret the omission of this information. We have added the list of features measured (Supplementary Data [[XXX]]), a description of the image analysis pipeline used to process the images (Supplementary Data [[XXX]]), and the actual image analysis pipeline that can be run in CellProfiler to exactly recreate the analysis (Supplementary Data [[XXX]]).  This information points to the identity of the algorithms used, which are, in turn, documented in CellProfiler's manual and open source code.  We have clarified the main text on the classifier algorithm: "... GentleBoosting applied to decision stumps", and added a note in the methods about the specific implementation: "..., based on code from Torralba, et al."
[[TODO: create all this Suppl data and specify which revision of CP was used; mention that old versions can be downloaded; put the reference to all this near the mention of "standard cytoprofiles"]]

COMMENT 3E: The description of the classifier evaluation is also inadequate, but extremely important for this type of publication.  How many individual images were evaluated by both the classifier and manually?  How were the test images presented to manual scorers chosen - were they selected by the classifier?  It is not possible to evaluate the false positive and false negative rates presented without a more thorough description of the evaluation procedure.  

Response:
Figure 2, and Figure 3 ("Validation" column), show the number of images that were scored by human scorers for each phenotype (e.g., 30 positive samples and 30 neutral samples for the Actin Blebs phenotype). Overall, 720 samples (360 positives, 360 neutrals) were scored by at least two humans (in addition to the computer). This, and the other questions raised, are now addressed more fully in the "Validation and comparison to previous methods" section: 

"We tested Classifier’s accuracy at ranking samples by having researchers score samples by eye. The results for Actin Blebs are shown in detail in Figure 2, and data for all phenotypes are shown in the Validation column in Figure 3. For each phenotype, Classifier rank-ordered the 5,000 puromycin-treated samples by Enrichment Score (Figure 2a), as would be done in a typical screen. For validation, researchers were forced to choose between pairs of samples: one sample in each pair had been scored by Classifier as positive (Enrichment Score greater than or equal 3) and the other as neutral (Enrichment Score roughly 0). The bar chart in Figure 2c indicates the number of times each sample was chosen as positive in these forced choices. Samples that Classifier scored as positives in this case (left, Figure 2c) were also chosen by the researchers as positives (11 or 12 times, out of 12 comparisons total), and none of the neutral samples (right, Figure 2c) was routinely chosen as positive (0 or 1 time out of 12 comparisons)."


COMMENT 3F: Lastly, it would be informative to compare the performance of this classifier to other existing classification methods.  What are the limitations of this technique? How many different phenotypes can be handled in the same experiment? How does the performance degrade when the number of rules increases? What types of phenotypes are undetectable by  Classify? 

Response:
For part of the answer to this query, please see our response above to Reviewer 2's comment 2A (Discussion of different classifier implementations). The 14 phenotypes we successfully screened in this study represent nearly every interesting, unusual phenotype we encountered in the images. Two exceptions are described in the manuscript, one of which (Peas in a Pod) was rescued by repeating image processing to measure an additional feature, and one of which (Sparkly Actin) was abandoned as described. Performance vs. number of rules is addressed by the new Supplementary Figure [[XXX]] described in our response to the next comment.


COMMENT 3G: A very real danger in applying pattern recognition techniques to visual assays is over-training.  In this application, this would manifest as an ability to robustly discern phenotypes within a set of images, but an inability to "generalize" this even to a repetition of the same experiment on a different day.  The performance of this classifier when applied to biological replicates should be looked into and discussed.

Response:
We add these new paragraphs describing additional experiments we now include to address these comments: 
"We found our approach resistant to over-training ([[Suppl. Figure XXX - cross validation results, see description below]]), as is generally the case for boosting algorithms ([[Ray TODO: provide reference]]). One strategy we used to prevent another type of over-training (training the system to recognize only a phenotypic subset of a broader phenotype of interest) is to rank-order samples after initial stages of machine learning, use Classifier to examine the negatively-classified cells marked in entire field-of-view images of the top "hits", and add additional positive examples to the training set to 'broaden' the definition of the phenotype. A similar approach can be even more readily used if positive control samples are available.

Still, none of these safeguards addresses whether a classifier will generalize to new experiments: classifiers trained on one experiment are unlikely to be applicable to experiments involving different assay protocols, cellular stains, or image acquisition instrumentation. In such cases, it will be necessary to create a new training set from scratch. Using our approach, the time required to do so is minimal relative to the effort involved in sample preparation and imaging for a large-scale screen. For replicates, the best approach (in addition to minimizing experimental variation and performing normalization if needed) is to create the training set using cells taken from all replicates, to maximize the classifier's robustness to any remaining experimental variation ([[Suppl. Figure YYY, Add experiments with training sets from piyush or jacob&milan or Eggert/Castoreno (probably latter).]]). This is because training a classifier on one replicate and applying it to another risks negatively impacting its accuracy due to undetected experimental variation."

[[Ray TODO: Create Suppl. Figure XXX - cross validation results: error rate vs. number of rules. Do not show sens/spec, just combined error rate or xvalidation margin. Legend should explain briefly that per-cell and per-image error rates are related, but in a way we don't necessarily understand]]

COMMENT 3H: A central criticism of this work is that the underlying basis is an image classification technique, but the paper focuses almost exclusively on its implementation in a software tool.  This is somewhat justified in that the software tool is indeed unique:  Despite previous work by others in this specific application of pattern recognition and image classification, no one has focused as much effort on making their software useable by bench scientists.  Whether this is sufficiently novel for publication in Nature Methods is an editorial decision.

Response:
Indeed, it was important to us to produce useable software to enable the approach we describe, rather than just a proof of principle. We also hope that the revised manuscript more clearly emphasizes, in response to the other reviewers' comments, that this is the first demonstration of the iterative machine learning approach to score multiple challenging and rare phenotypes in large-scale screens.


COMMENT 3I: Additional detailed comments:
COMMENT 3I.1: Page 5. It is mentioned that 611 measurements are used without mention of what these are or even what types of measurements are involved. Some analysis could also be useful, but at the very least users should be able to know what is being measured and how. I couldn't find them in the enclosed additional papers, and also not in the CellProfiler paper published in "Genome Biology". The image analysis is one of the most important contributions of this work, and it must be described. Related to the previous comment. In page 4 ("Results"), several different characteristic are measured as a first step of the analysis. However, except from specifying that these include size, shape, intensity and textures, no information describing these measurements is provided. For instance, what characteristics of the shapes are used? How are they measured? The reference (27) does not lead to a source that specifies that. Intensity can be measured by very many different ways. Is it measured by using the statistical properties of the pixel intensity distribution? If so, which statistical properties are used? How are they measured? The term "texture" is also a very broad definition. Many techniques for measuring textures have been proposed, and the paper must describe which methods are used and how, or at least include references in which the exact same fashion of the use of these textures is specified. The rule files also feature some other measurements such as edges and polynomial decomposition. These should also be described in the text...RNAi screens for diverse phenotypes using Classifier: What are the "standard cytoprofiles"? Is there a list of the cell descriptors?

Response:
The revised manuscript now addresses these questions - please see our response to Reviewer 2's comment 2A (c: Examination of feature space explored by different phenotypes) and also our response to Reviewer 3's comment 3D. As well, we moved reference 27 because it refers to the coining of the term "cytoprofile", not information about specific measurements.


COMMENT 3I.2: Another major point is the rules. It is clear that the rules are determined by the researcher in an interactive fashion using trial and error until she is satisfied with the results.  These rules, as provided in supplementary materials need further explanation.  The rule files seem to be a set of "if" conditions. However, the manuscript does not describe how the if clauses are handled, and it is not clear how the inference computation is performed and how the ranking is determined.  It is also not clear how important the manual correction of the rules is, or if the rules can be used without manual correction. What is the contribution of the manual correction (in terms of classification accuracy) to the automatically generated rules. This point is important because the readers might be curious to know if the method can be fully automated.

[[Anne TODO: we should also include a 'translation' of an individual rule into English, either in the methods or the Suppl Data file that shows the top rules for each phenotype.]]

Response:
[[Ray, is this answered sufficiently by response to comment 3D? You might elaborate on that response further if there's a bit more queried in this comment, then say "We now address these questions above - please see our response to comment 3D".  The last 3 sentences about whether it could be fully automated brings us back to the Bakal-like method, so this query needs to have an additional response, something like "Regarding whether the method can be fully automated, our response to comment 3A above now addresses the fact that while some screens can indeed be fully automated (and the Classifier function of CellProfiler Analyst for the first time readily enables this kind of analysis for researchers without programming skills), the main purpose of the interactive approach we present is for cases when such analysis fails."]]


[[Ray: "Manual correction" - I'm not sure where that idea came from, perhaps the CPA manual?  It's not something we talk about in the paper, is it?]]

COMMENT 3I.3: Just by looking at the rule files, it seems that some of the image content descriptors that are used are computationally intensive. A few words about the computational resources that are required and the response time of the system might be useful for researchers who consider using this method, especially on the very large datasets (10s - 100s of thousands of images) one would encounter in a screen.

Response:
The revised manuscript now addresses these questions - please see our response to Reviewer 2's comment 2C.1. 


COMMENT 3I.4: Page 10:  "50 individual rules" ==> "Fifty individual rules". (A sentence should not start with a number). Illustrations: All graphs must have clear units on both axes.

Response:
Done.
[[TO DO: need to figure out which figures are referenced and fix them. 

Ray: I think they mean every subplot.  I would push back, if so.]]

COMMENT 3I.5: It is totally unclear how gentleboosting is used. Boosting requires ground truth and an existing classifier (as opposed to filtering, which requires only ground truth). Since the boosting is applied here as a first step of defining the classifier rules, it is unclear what classifier is used by gentleboosting (decision trees?). It should also be mentioned that the classifier used for boosting is different than the one used for the actual classification, and therefore features that are selected might not be informative for the actual classifier. Under these circumstances, it is unclear why the authors chose to use slow boosting over much faster and classifier-independent filtering.  The number of terms used by gentleboosting is also unclear. Is this number fixed or is it determined dynamically in some way? While gentleboosting assigns weights to the features, it is unclear how the tentative rule is generated.  Also, boosting is usually a computationally intensive task. A note about this issue is also required.

Response:
We have clarified which weak learner is used (regression stumps) in the GentleBoosting algorithm.  Please see our response to comment 3D.  The boosted classifier is made up of multiple stumps, which provides some information about relevant features, though their combination is more complex than any single feature.  The performance of training of and scoring via the classifier is addressed in our response to Reviewer 2's comment 2C.1.  More detail is given about the rule selection via the reference to Torralba, et al. (see also response to comment 3D).  

[[Ray: I'm not sure what they mean by classifier-independent filtering.]]



COMMENT 3I.6: Page 5: "Few dozens" is a rough estimation.  How does a researcher determine the number? Usually, the performance of machine learning algorithms can be improved by using more samples for training. This trivial practice of machine learning might not be known to many biologists. To help the readers effectively use the described tool, this must be further discussed, preferably by measuring the effect of several different training-set sizes on classifier performance.

Response:
While the manual provides practical tips like this, we agree this issue is important enough to mention in the manuscript. The revised manuscript now addresses these questions as follows (after the sentence, "After a few dozen cells are scored, the researcher can begin the iterative machine learning phase, where the computer generates a tentative rule based on the scored cells and presents the researcher with cells scored according to that rule"):

"While more is always better when considering how many cells to score before beginning the iterative machine learning phase (if too few are used to initiate machine learning, the computer may train itself to a too-narrow definition of the phenotype), generating a large training set in the initial stage can be difficult when the phenotype is rare or no positive control samples are available; in fact, these are the cases where the iterative nature of Classifier is necessary. The optimal initial training set depends on the complexity of the phenotype and the scarcity of positive cells in the experiment, but a good rule of thumb is to begin with at least 50 [[or, perhaps this is where we reference a new Suppl figure - to address the last sentence of the comment, it'd be super easy to make a Suppl figure showing how a phenotype (e.g. Crescents, because it's got a large training set?) responds to increased training set size by choosing random training sets and measuring cross validation accuracy on the entire training set. That'd be kind of neat and I'm curious to see the results myself.]]"


[[Ray: can we shorten this?]]


COMMENT 3I.7: The term "scoring" is used throughout the manuscript in the sense of associating a specific sample with one of a discrete set of classes. This, however, is actually "classification", while the term "scoring" implies on some continuos indicator, e.g., Z-scores, Fisher scores, etc.

Response:
We have adjusted the text to use the terms "classify" to mean label a cell as positive or negative based on some learned rules, and "score" to mean assign a continuous Enrichment Score to a particular sample. [[TO DO.]]

[[Ray - what do you think? I bet for Enrichment score we really do mean 'score', but for 'scoring' positives and negatives we really mean classify in many places in the text (even though the classification is based on a continuous score). This is your judgment whether (a) the comment is totally valid, (b) it's a matter of preference but we may as well change it because it's easy, or (c) it's worth arguing that our original terminology is reasonable. I'd vote for just changing the text because it's simple. LMK.

Ray: I think we change the text.]]


COMMENT 3I.8: Discussion: Since only a subset of the dataset (reference 12) was used, the reader might wonder what was the barrier for repeating the same experiment for much larger network than the 14 phenotypes. What were the criteria for choosing these phenotypes.

Response:
[[Ref 12 is the Moffat paper. I'm not sure what the query is - they think we used fewer than the number of samples in the Moffat paper (which is only true in the sense of puro+ instead of all)? But then the query says 'much larger network than the 14 phenotypes'. Not sure what that means. For the phenotypes, we could refer to our response to comment 3F "The 14 phenotypes we successfully screened in this study represent nearly every interesting, unusual phenotype we encountered in the images."]]

[[Note, the original sentence is: "We used the samples treated with puromycin (the selection agent for the shRNA vector) for the validation step shown in Figure 3." and we could easily add "... because XXX (I don't remember why you decided that)."]]

[[Ray: How about: "... for two reasons: that we expect puromycin to affect phenotype penetrance in the wild type population, and because puromycin selection culls cells where the shRNA vector failed to infect, leading to more homogeneous populations in each sample."]]


COMMENT 3I.9: Page 9: The statement according which Classifier can be used on full organisms such as zebrafish and C. elegans is not supported by any empirical or theorethical evidence. The authors can use some of the publicly available datasets and report on the detection rate. Unless this is done, such a statement should be left out of the manuscript.

Response:
We have omitted all text at the end of the discussion describing future possibilities, including application of the approach to whole organisms.


COMMENT 3I.10: Validation and comparison to previous methods: It is clear that Classifier was compared to human detection, but the way the human detection was performed is not clear. Was each sample classified by a single researcher? What is the error rate of this manual classification? How many samples were used for training the classifier? What efforts (human resources) were required to train the classifier.

Response:
These questions have now been addressed - see response to comments [[TO DO: X Y Z]].


COMMENT 3I.11: The motivation for the forced choice test is unclear. Is there a specific reason for using this kind of test, rather than simply counting the number of samples that their automated score was in agreement with the manual score?

Response:
We have added explanation to the text: "The biologically relevant score for a sample is enrichment of cells that display the phenotype, rather than a hard 'positive' or 'negative' label, because these samples, as in most screens (REF: perrimon/friedman article that talks about screens yielding smooth distributions rather than sharply defined positives), do not cleanly fall into clear positive and negative classes. Because our goal is to bring highly enriched samples to the attention of the researcher, our aim is to ensure that top-ranking samples are indeed enriched relative to samples scored as neutral. Scoring two samples relative to each other thus avoids the complication that different researchers will subconsciously choose different 'thresholds' at which they call a sample positive or negative."

[[Ray, do you find this convincing? Perhaps you also have a more technical response to add to this one? Feel free to adjust.

Ray: I have wordsmithing ideas, but they can wait until Monday.]]


COMMENT 3I.12: Also, a new descriptor (the angle between the two nearest neighbors) was added. It is not clear, however, if and how the user can add customized descriptors.

Response:
We have added this information to the main text: "If additional features are predicted to be useful for a particular phenotype, they can be added in two ways: (1) Additional segmentation and feature extraction modules can be added to the image analysis pipeline in CellProfiler. For example, this approach could be used to define a new compartment of interest that might be helpful to score a particular phenotype, like a concentric ring from 2-6 microns outside the nucleus. Measure modules could then measure the size, shape, intensity or texture for that newly defined compartment.  Or, additional texture-measuring modules can be added, each one probing a different scale of texture. This does not require programming but rather simply adding modules to the pipeline and adjusting settings. (2) By adjusting the source code of CellProfiler, new features can be added to existing feature extraction modules or new modules can be written from scratch. This is the approach we took when we added code to measure the angle between neighbors, because it was not a pre-existing measurement made by CellProfiler's Measure Object Neighbors module. It is now a measurement included in the standard cytoprofile for future screens."

[[Ray: I would like to wordsmith this on Monday.  Currently too long.]]

COMMENT 3I.13: Page 3: "Machine learning methods that select and combine multiple features for automated cell scoring have been explored and used for a few particular phenotypes"  The cited papers do not support the statement. Previous efforts (e.g., Neumann et al., Bakal et al., Orlov et al., Chen & Murphy) have resulted in general phenotype recognition tools that can handle a large number of phenotypes, rather than focusing on detection of a few particular phenotypes. This is an important point, and it should be clear to the reader that this work is not the first attempt of general automated phenotype recognition.

Response:
These questions have now been addressed - see response to Reviewer 1's comment 1A.
[[Anne TO DO: Check whether that's correct and search this response for other places we address this (search for 'explored')]]

[[Anne TODO: Check editor comments; here we have just addressed reviewer comments]]